{
  "name": "s3-streaming-upload",
  "description": "Streaming upload to S3",
  "version": "0.1.13",
  "author": {
    "name": "Apiary Inc",
    "email": "support@apiary.io"
  },
  "contributors": [
    {
      "name": "Lukas Linhart",
      "email": "lukas@apiary.io"
    }
  ],
  "dependencies": {
    "async": "",
    "aws-sdk": ""
  },
  "devDependencies": {
    "coffee-script": "~1.6.3",
    "chai": "",
    "mocha": "",
    "sinon": "",
    "streamers": ""
  },
  "keywords": [
    "aws",
    "s3",
    "stream",
    "aws stream",
    "aws s3 stream",
    "aws s3",
    "aws s3 streaming",
    "aws s3 streaming upload",
    "aws-sdk s3 streaming upload",
    "aws s3 upload",
    "s3 upload",
    "s3 stream upload",
    "s3 streaming",
    "streaming upload",
    "s3 streaming upload"
  ],
  "scripts": {
    "prepublish": "scripts/build",
    "test": "./node_modules/.bin/mocha --compilers \"coffee:coffee-script\""
  },
  "licenses": [
    {
      "type": "MIT",
      "url": "https://github.com/apiaryio/s3-streaming-upload/blob/master/LICENSE"
    }
  ],
  "repository": {
    "type": "git",
    "url": "http://github.com/apiaryio/s3-streaming-upload"
  },
  "bugs": {
    "url": "http://github.com/apiaryio/s3-streaming-upload/issues"
  },
  "main": "lib/",
  "engines": {
    "node": "*"
  },
  "readme": "## s3-streaming-upload [![Build Status](https://travis-ci.org/apiaryio/s3-streaming-upload.png?branch=master)](https://travis-ci.org/apiaryio/s3-streaming-upload)\n\n[s3-streaming-upload](https://github.com/apiaryio/s3-streaming-upload) is [node.js](http://nodejs.org) library that listens to your [stream](http://nodejs.org/docs/v0.8.9/api/stream.html) and upload its data to Amazon S3 using [MultiPartUpload API](http://docs.amazonwebservices.com/AmazonS3/latest/dev/sdksupportformpu.html).\n\nIt is heavily inspired by [knox-mpu](https://github.com/nathanoehlman/knox-mpu), but unlike it, it does not buffer data to disk and is build on top of [official AWS SDK](https://github.com/aws/aws-sdk-js) instead of knox.\n\n### Installation\n\nInstallation is done via NPM, by running ```npm install s3-streaming-upload```\n\n### Features\n\n* Super easy to use\n* No need to know data size beforehand\n* Stream is buffered up to specified size (default 5MBs) and then uploaded to S3\n* Segments are not written to disk and memory is freed as soon as possible after upload\n* Uploading is asynchronous\n* You can react to upload status through events\n\n\n### Quick example\n\n```javascript\n\nvar Uploader = require('s3-streaming-upload').Uploader,\n    upload = null,\n    stream = require('fs').createReadStream('/etc/resolv.conf');\n\nupload = new Uploader({\n  // credentials to access AWS\n  accessKey:  process.env.AWS_API_KEY,\n  secretKey:  process.env.AWS_SECRET,\n  bucket:     process.env.AWS_S3_TRAFFIC_BACKUP_BUCKET,\n  objectName: \"myUploadedFile\",\n  stream:     stream\n});\n\nupload.on('completed', function (err, res) {\n    console.log('upload completed');\n});\n\nupload.on('failed', function (err) {\n    console.log('upload failed with error', err);\n});\n````\n",
  "readmeFilename": "README.md",
  "_id": "s3-streaming-upload@0.1.13",
  "_from": "s3-streaming-upload@*"
}
